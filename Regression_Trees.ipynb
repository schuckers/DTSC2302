{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa7b096",
   "metadata": {},
   "source": [
    "##  Decision Trees for Regression\n",
    "\n",
    "We can build decision trees for regression just like we built regression trees for classification.  The primary difference is the response is numeric for a regression tree.  This means that trees are split on a different metric than *gini impurity* which we used for classification trees.  Below we will use *mean squared error* which is the default to make our splits/branches but there are other choices available such as *mean absolute error*.\n",
    "\n",
    "One thing to emphasis about decision trees is that the features much be numeric.  That means if we have categorical features/predictors/covariates that we will have to create indicators or one-hot encoded values for those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4905334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import colors\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data to dataframe called ames\n",
    "ames = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2301/Data/Ames_house_prices.csv\", na_values=['?'])\n",
    "# replace the ? in the data with NaN for missing values\n",
    "ames.replace([' ?'],np.nan)\n",
    "# show information about the dataframe\n",
    "ames.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a075f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=ames['SalePrice']\n",
    "#ames=ames.drop('SalePrice', axis=1)\n",
    "# we have to have numeric predictors for a decision tree\n",
    "X=ames[['LotFrontage','LotArea','ScreenPorch','MoSold','YearBuilt','YearRemodAdd', 'BsmtFinSF1','OpenPorchSF','PoolArea','1stFlrSF','2ndFlrSF','GrLivArea']]\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a regression tree model\n",
    "dtree = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "# Train the model\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = dtree.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot of the decision tree\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "tree.plot_tree(dtree,\n",
    "                   feature_names=['LotFrontage','LotArea','ScreenPorch','MoSold','YearBuilt','YearRemodAdd', 'BsmtFinSF1','OpenPorchSF','PoolArea','1stFlrSF','2ndFlrSF','GrLivArea'],\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497cba21",
   "metadata": {},
   "source": [
    "Whoo-boy. That is a tree.  Perhaps we should prune that tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed374fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a decision tree with a max depth of 4\n",
    "dt_pre_pruned = DecisionTreeRegressor(max_depth=4, min_samples_split=5, min_samples_leaf=2)\n",
    "\n",
    "# Train the model\n",
    "dt_pre_pruned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180fb91d",
   "metadata": {},
   "source": [
    "Let's get the feature importance for this model.  We'll use permutation importance for this model on the test data.\n",
    "\n",
    "Reminder that the permutation importance is the amount that the model performance changes if we permute the values in that feature while keeping other features intact.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation feature importance\n",
    "\n",
    "result = permutation_importance(dt_pre_pruned, X_test, y_test, n_repeats=10, random_state=0, n_jobs=-1)\n",
    "perm_imp_df = pd.DataFrame({'Feature': feature_names, 'Permutation Importance': result.importances_mean}).sort_values('Permutation Importance', ascending=False)\n",
    "print(perm_imp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38e97c8",
   "metadata": {},
   "source": [
    "The values we get here make a good bit of sense as to the value of a house from my perspective.  *GrLivArea* is the amount of above grade (above ground?) living space and\n",
    "the age of a house, *YearBuilt*, certainly have impacts on prices for houses.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead8867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = dt_pre_pruned.predict(X_test)\n",
    " \n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca798d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "tree.plot_tree(dt_pre_pruned,\n",
    "                   feature_names=['LotFrontage','LotArea','ScreenPorch','MoSold','YearBuilt','YearRemodAdd', 'BsmtFinSF1','OpenPorchSF','PoolArea','1stFlrSF','2ndFlrSF','GrLivArea'],\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89551a29",
   "metadata": {},
   "source": [
    "Our out of sample performance is not as good but let's try something smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9003ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pre_pruned2 = DecisionTreeRegressor(max_depth=2, min_samples_split=5, min_samples_leaf=2)\n",
    "\n",
    "# Train the model\n",
    "dt_pre_pruned2.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "predictions = dt_pre_pruned2.predict(X_test)\n",
    " \n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe9084",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "tree.plot_tree(dt_pre_pruned2,\n",
    "                   feature_names=['LotFrontage','LotArea','ScreenPorch','MoSold','YearBuilt','YearRemodAdd', 'BsmtFinSF1','OpenPorchSF','PoolArea','1stFlrSF','2ndFlrSF','GrLivArea'],\n",
    "                   filled=True)\n",
    "feature_names=['LotFrontage','LotArea','ScreenPorch','MoSold','YearBuilt','YearRemodAdd', 'BsmtFinSF1','OpenPorchSF','PoolArea','1stFlrSF','2ndFlrSF','GrLivArea']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9b1069",
   "metadata": {},
   "source": [
    "So with that depth we did not get a better tree but we did get one we can visualize.\n",
    "\n",
    "Let's look at the cross-validation rather than a single test set.  We'll use 8-fold CV and we'll look at trees with depth 2 and depth 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f5ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pre_pruned3 = DecisionTreeRegressor(max_depth=2, min_samples_split=5, min_samples_leaf=2)\n",
    "cv_scores_tree3 = cross_val_score(dt_pre_pruned3, X, y, cv=8, scoring='neg_root_mean_squared_error')  # 8-fold cross-validation\n",
    "print(f\"RBF Kernel SVM cross-validation accuracy: {cv_scores_tree3.mean() :.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae6800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pre_pruned3 = DecisionTreeRegressor(max_depth=4, min_samples_split=5, min_samples_leaf=2)\n",
    "cv_scores_tree3 = cross_val_score(dt_pre_pruned3, X, y, cv=8, scoring='neg_root_mean_squared_error')  # 8-fold cross-validation\n",
    "print(cv_scores_tree3)\n",
    "print(f\"RBF Kernel SVM cross-validation accuracy: {cv_scores_tree3.mean() :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92b18ea",
   "metadata": {},
   "source": [
    "So from above we can see that the larger tree with *max_depth=4* performed better on cross-validation.  Below is code to determine the\n",
    "best *max_depth* via cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd63110",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = 12\n",
    "\n",
    "# create an empty array to store the calculated mean RMSEs\n",
    "aver_rmse = []\n",
    "depth=list(range(1,13,1))\n",
    "# loop through the different pruning sizes\n",
    "for i in range(sizes):\n",
    "  # create a pruned tree of size i+1\n",
    "  dt_pruned = DecisionTreeRegressor(max_depth=i+1, min_samples_split=5, min_samples_leaf=2)\n",
    "  # calculate the negative RMSE, why python insists on the negative here I do not know\n",
    "  # ...\n",
    "  # well...\n",
    "  # actually I have a guess.  I'm guessing that the algorithm is set to maximize a quantity\n",
    "  # such as r^2.  And maximizing -RMSE is the same as minimizing RMSE\n",
    "  cv_scores_tree = cross_val_score(dt_pruned, X, y, cv=8, scoring='neg_root_mean_squared_error')\n",
    "  # get the average from the 8-fold cross validations and multiply by -1 to get back to the right scale. \n",
    "  avg = -1*cv_scores_tree.mean()\n",
    "  # add avg to the list of other bootstrapped means\n",
    "  aver_rmse.append(avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e217b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the values\n",
    "print(aver_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035598e",
   "metadata": {},
   "source": [
    "Let's look at a plot of these cross validated RMSE's to get a better sense of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9893c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a scatterplot for depth vs average RMSE\n",
    "ax=sns.scatterplot(x=depth, y=aver_rmse)\n",
    "# \"ax\" is the conventional name.\n",
    "ax.set_title('Plot of CV RMSE by pruned depth')\n",
    "ax.set_ylabel('CV RMSE')\n",
    "ax.set_xlabel('prune depth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc37c13a",
   "metadata": {},
   "source": [
    "It looks like the 'best' performing prune depth is 7 or 8 or 9.  I'd prefer the smallest model here so that seems to be *7*.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07828b6",
   "metadata": {},
   "source": [
    "### Forests from Trees\n",
    "\n",
    "The next cells of code move from regression decision trees to regression random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb1a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train Random Forest Regressor with maximum depth of 3 \n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "# get the 8 fold cross validation score\n",
    "scores = -1*cross_val_score(rf_regressor, X, y, cv=8, scoring='neg_root_mean_squared_error')\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6fd9b2",
   "metadata": {},
   "source": [
    "So the above output was from making many trees with just a depth of 3.  Note that the average RMSE from our plot above with a prune depth of *3* was about 50000.  We got a good bit smaller RMSE by using a random forest of the same size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c29f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Regressor with maximum depth of 3 \n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=4, random_state=42)\n",
    "# get the 8 fold cross validation score\n",
    "scores = -1*cross_val_score(rf_regressor, X, y, cv=8, scoring='neg_root_mean_squared_error')\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a5a77b",
   "metadata": {},
   "source": [
    "From that result, we can see that a random forest with a depth of *4* continues our improvement over having a depth of *3*.  In the Tasks below, you'll create a loop for\n",
    "determining the 'best' value based upon cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c38144",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. Write code to determine the best *max_depth* for a regression random forest for these data.  Make a plot like we did above to visualize the output.\n",
    "\n",
    "2. Determine the permutation importance for the best regression random forest based upon your results from above.\n",
    "\n",
    "3. Choose a subset of features/predictors from your results in Task 2.  Rerun the code for Tasks 1 and 2 with these new predictors.  Does your CV RMSE change?  If so, how?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
