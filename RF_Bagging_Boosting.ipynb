{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests, Bagging and Boosting\n",
    "In this Jupyter Notebook we'll introduce the concepts of Bagging and Boosting and we'll look at Random Forests which are a form of bagging.\n",
    "\n",
    "The idea of bagging is to create a model that is an average of a bunch of other smaller models.  We do this by bootstrapping the data and building a smaller model (think trees without much depth or logistic regression models with only a small number of predictors, say 2 or 3) on that data. Then repeating that process so you get lots of small models and you average all of those to get your predictions.\n",
    "\n",
    "A random forest is a particular form of bagging by which you build small trees by randomly sampling observations and randomly subsetting features.  \n",
    "\n",
    "Boosting is a way to improve model performance by effectively bumping up or 'boosting' the value that the model places on residuals or observations that are misclassified.\n",
    "\n",
    "These methods can be computationally intensive for larger data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the _iris_ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging, Boosting and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So below are three different  approaches below.\n",
    "\n",
    "The first is bagging using Logistic Regression so that the basic classifier is a logistic regression.  That estimator can be replaced with other\n",
    "types of classifiers.  The default is a decision tree classifier.\n",
    "\n",
    "The second is a classifier that uses XGBoost, so that is boosting.  The third is a random forest.\n",
    "\n",
    "We'll start by looking at performance on the test set generated above for the _iris_ data.'\n",
    "\n",
    "The argument *n_estimators* is the number of models being generated by the bagging process.  For a Random Forest, it is the number of trees in the forest. \n",
    "Making this a large number can drastically increase the amount of time your code will take to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Bagging Classifier (using Decision Trees as base estimators)\n",
    "bagging = BaggingClassifier(estimator=LogisticRegression(max_iter=200),n_estimators=100, random_state=42)\n",
    "bagging.fit(X_train, y_train)\n",
    "bagging_preds = bagging.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test, bagging_preds)\n",
    "print(\"Bagging Classifier Accuracy:\", bagging_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost Classifier\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_preds)\n",
    "print(\"XGBoost Accuracy:\", xgb_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_preds)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classifiers do really well on these data.\n",
    "\n",
    "So let's move to the Penguins data.  We are going to try classifying whether or not a Penguin is a female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2301/Data/penguins.csv\", na_values=['NA'])\n",
    "# remove rows with missing data\n",
    "penguins.dropna(inplace=True)\n",
    "penguins.head()\n",
    "penguins['sex01'] = (penguins['sex']==\"female\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "np.random.seed(4242)\n",
    "X = penguins[['bill_length_mm',\t'bill_depth_mm',\t'flipper_length_mm',\t'body_mass_g']]\n",
    "y=penguins['sex01']\n",
    "\n",
    "feature_names=['bill_length_mm',\t'bill_depth_mm',\t'flipper_length_mm',\t'body_mass_g']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Bagging Classifier (using Decision Trees as base estimators)\n",
    "bagging = BaggingClassifier(estimator=LogisticRegression(max_iter=200), n_estimators=100, random_state=42)\n",
    "bagging.fit(X_train, y_train)\n",
    "bagging_preds = bagging.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test, bagging_preds)\n",
    "print(\"Bagging Classifier Accuracy:\", bagging_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost Classifier\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_preds)\n",
    "print(\"XGBoost Accuracy:\", xgb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_preds)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, of course, we have learned not to judge a model's performance on how it does on a single train/test split.  So let's do some cross-validation.\n",
    "\n",
    "For *cross_val_score*, if we pass an _int_ or _None_ to *cv* as an input and the estimator is a classifier and y is either binary or multiclass, _StratifiedKFold_ is used. \n",
    "\n",
    "What is _StratifiedKFold_ you ask?  _StratifiedKFold_ is a way of generating the folds by stratified sampling so that the proportion from each class is the same.  For example,\n",
    "suppose that we had 70% female penguins in our data.  Using stratified sampling, _StratifiedKFold_ would ensure that each of our folds for cross-validation would also have 70% female penguins.  Slick, right?\n",
    "\n",
    "Note that doing the cross validation below will take some time because with each fold we are fitting 100 different models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "bagging_pens = BaggingClassifier(estimator=LogisticRegression(max_iter=200),n_estimators=100, random_state=42)\n",
    "cv_scores_rbf = cross_val_score(bagging_pens, X, y, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "print(f\"Logistic Bagging cross-validation accuracy: {cv_scores_rbf.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pens=xgb.XGBClassifier(n_estimators=100)\n",
    "cv_scores_rbf = cross_val_score(xgb_pens, X, y, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "print(f\"XGBoost cross-validation accuracy: {cv_scores_rbf.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pens=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "cv_scores_rbf = cross_val_score(rf_pens, X, y, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "print(f\"Random Forest cross-validation accuracy: {cv_scores_rbf.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the LogisticRegression takes awhile to fit 100 models but it does slightly better than the other two models in this case.  \n",
    "\n",
    "Let's look at the Importance of variables for the bagging logistic model using Permutation Importance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed so we have similar\n",
    "np.random.seed(250402)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "#Training the Model\n",
    "bagging_pens.fit(X_train_scaled, y_train)\n",
    "#Predicting on Test Set and Evaluating the Model\n",
    "y_pred = bagging_pens.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the BaggingClassifier: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Permutation Importance\n",
    "perm_importance = permutation_importance(bagging_pens, X_test_scaled, y_test, n_repeats=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Feature Importance\n",
    "\n",
    "features = data.feature_names\n",
    "\n",
    "# Plotting the permutation importance results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(features, perm_importance.importances_mean, color='orange')\n",
    "plt.title(\"Permutation Importance from Logistic BaggingClassifier\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recall that the Permutation Importance is based upon a single test set and may change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. Change the line above that is 'np.random.seed(250402)' and replace that with another seed.  How does that change the permutation importance values?\n",
    "\n",
    "2. Repeat the above code on permutation importance for the XGBoost Model and the Random Forest Model.  How do your results differ?\n",
    "\n",
    "3. Open the breast cancer data, and fit an XGBoost and a Random Forest model to those data.  How well do those models do with 8-fold cross validation?  Which is better?\n",
    "\n",
    "4. Determine the permutation importance of the features from the XGBoost and the Random Forest.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
