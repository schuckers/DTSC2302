{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees and Support Vector Machines\n",
    "\n",
    "In this notebook, we'll look at decision trees and support vector machines for classification.  Decisions trees split data along a single variables in a binary way, that is into two parts.  The goal is to get to leafs or nodes that are relatively homogenous because homogeneity makes for better prediction.  The trees here will use a Gini impurity index.  \n",
    "\n",
    "Support vector machines (or support vector classifiers) are classifiers that try to find decision boundaries that have some separation between the classes.  This is similar to what LDA or QDA do but with SVM's the tradeoff is made of trying to find more robust separation, wider regions of separation between the classes, in exchange for making a some errors.  The hope is that the wider regions of separation will yield better out of sample or cross validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries that we need\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import colors\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "# Introduction to Support Vector Machines (SVM) for Classification\n",
    "\n",
    "# Let's start by importing the necessary libraries\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing confusion matrices for each classifier\n",
    "# Here's another function for plotting the confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the bcancer data\n",
    "bcancer = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2301/Data/BreastCancer.csv\", na_values=['NA'])\n",
    "bcancer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the features X and target y\n",
    "X=bcancer[['Concavity','Texture','Radius','Area']]\n",
    "y = bcancer['Diagnosis']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "dtree = DecisionTreeClassifier(random_state=420)\n",
    "\n",
    "# Train the classifier\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = dtree.predict(X_test)\n",
    "\n",
    "# get the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Evaluate the model with out of sample prediction\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a visualization of the performance results that is a bit easier to digest.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Concavity','Texture','Radius','Area']\n",
    "target_names = ['B','M']\n",
    "plot_confusion_matrix(y_test, y_pred, 'Decision Tree Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the tree itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot of the decision tree\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "tree.plot_tree(dtree,\n",
    "                   feature_names=['Concavity','Texture','Radius','Area'],\n",
    "                   class_names=['B','M'],\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree above is quite deep. Looks like it has a depth of 10 splits.  That's quite a few.  \n",
    " It is also possible, maybe even likely, that the tree is overfit given how many different branches/splits there are and the depth of the tree.\n",
    "\n",
    "Let's look at a version that is pruned.  We'll start with a tree that has *max_depth* of 4.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pre_pruned = DecisionTreeClassifier(max_depth=4, min_samples_split=5, min_samples_leaf=2)\n",
    "\n",
    "# Train the model\n",
    "dt_pre_pruned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plot of that tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "tree.plot_tree(dt_pre_pruned,\n",
    "                   feature_names=['Concavity','Texture','Radius','Area'],\n",
    "                   class_names=['B','M'],\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "And here's the test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt_pre_pruned.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, 'Decision Tree Confusion Matrix (pruned)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance has gone up slightly.  Let's try pruning further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pre_pruned2 = DecisionTreeClassifier(max_depth=2, min_samples_split=5, min_samples_leaf=2)\n",
    "\n",
    "# Train the model\n",
    "dt_pre_pruned2.fit(X_train, y_train)\n",
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "tree.plot_tree(dt_pre_pruned2,\n",
    "                   feature_names=['Concavity','Texture','Radius','Area'],\n",
    "                   class_names=['B','M'],\n",
    "                   filled=True)\n",
    "\n",
    "y_pred = dt_pre_pruned2.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, 'Decision Tree Confusion Matrix (pruned)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is a better tree by far than the previous.  We can also clearly see which of the features is important for this classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Linear Kernel SVM\n",
    "\n",
    "# Create a linear kernel SVM model\n",
    "linear_svm = SVC(kernel='linear')\n",
    "\n",
    "# Train the model on the training data\n",
    "linear_svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_linear = linear_svm.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
    "print(f\"Linear SVM accuracy: {accuracy_linear * 100:.2f}%\")\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_linear, 'Decision Tree Confusion Matrix (pruned)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector machine we used above creates a linear decision boundary.  \n",
    "\n",
    "Below we will look at the Radial Basis Function (RBF) Kernel which allows for distance based decision boundaries.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an RBF kernel SVM model\n",
    "rbf_svm = SVC(kernel='rbf')\n",
    "\n",
    "# Train the model on the training data\n",
    "rbf_svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rbf = rbf_svm.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "print(f\"RBF Kernel SVM accuracy: {accuracy_rbf * 100:.2f}%\")\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_rbf, 'Decision Tree Confusion Matrix (pruned)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have just used a single split to evaluate the performance of our models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cross-validation for linear kernel\n",
    "cv_scores_linear = cross_val_score(linear_svm, X, y, cv=5)  # 5-fold cross-validation\n",
    "print(f\"Linear Kernel SVM cross-validation accuracy: {cv_scores_linear.mean() * 100:.2f}%\")\n",
    "\n",
    "# Cross-validation for RBF kernel\n",
    "cv_scores_rbf = cross_val_score(rbf_svm, X, y, cv=5)  # 5-fold cross-validation\n",
    "print(f\"RBF Kernel SVM cross-validation accuracy: {cv_scores_rbf.mean() * 100:.2f}%\")\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *rbf* kernel that we use here is a radial basis function and as the name suggests it is a function of $$exp^d$$ where $d$ is the Euclidean distance/radial distance between two vectors.  This allows for the creation of decision boundaries that are based upon the \n",
    "\n",
    "\n",
    "[<https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html>]\n",
    "\n",
    "[<https://en.wikipedia.org/wiki/Radial_basis_function_kernel>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks \n",
    "\n",
    "1.   Using Texture, Radius, Area, Compactness and Smoothness, run 8 fold cross validation for the following models: logistic regression, SVM with linear kernel, SVM with RBF kernel, decision tree with depth of 3, decision tree with depth of 5.  Report which method did performed the best.\n",
    "\n",
    "2. Write a paragraph summarizing the analysis you did in Task 1 and explaining the model to a classmate.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
