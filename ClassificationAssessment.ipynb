{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Performance Assessment\n",
    "\n",
    "Classification performance is different from regression performance.  Below we look at some of the calculations that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate specificity.\n",
    "\n",
    "    Args:\n",
    "        y_true (list or numpy.ndarray): True labels.\n",
    "        y_pred (list or numpy.ndarray): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        float: Specificity score.\n",
    "    \"\"\"\n",
    "    true_negatives = sum((y_true == 0) & (y_pred == 0))\n",
    "    false_positives = sum((y_true == 0) & (y_pred == 1))\n",
    "    \n",
    "    if (true_negatives + false_positives) == 0:\n",
    "      return 0.0\n",
    "    \n",
    "    specificity = true_negatives / (true_negatives + false_positives)\n",
    "    return specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply these performance metrics to the Overdrawn data and models we fit last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the monkey data\n",
    "overdrawn = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2302/Overdrawn.csv\")\n",
    "# get info about these data\n",
    "overdrawn.dropna(inplace=True)\n",
    "overdrawn.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is about students.  Here are the details on the variables in these data:\n",
    "\n",
    "_Age_ =\tAge of the student (in years)\n",
    "\n",
    "_Sex_ =\t0=male or 1=female\n",
    "\n",
    "_DaysDrink_ = Number of days drinking alcohol (in past 30 days)\n",
    "\n",
    "_Overdrawn_\t= Has student overdrawn a checking account? 0=no or 1=yes\n",
    "\n",
    "The first regression that we will do will use _DaysDrink_ as a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center _Age_\n",
    "overdrawn['Age']=overdrawn['Age']-overdrawn['Age'].mean()\n",
    "\n",
    "overdrawn_X = overdrawn[['DaysDrink','Age','Sex']]\n",
    "overdrawn_X = sm.add_constant(overdrawn_X)\n",
    "\n",
    "overdrawn_y = overdrawn['Overdrawn']\n",
    "model2_sm = sm.Logit(overdrawn_y, overdrawn_X).fit()\n",
    "\n",
    "# Print the summary table, which includes p-values\n",
    "print(model2_sm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create y_true which is 'ground truth' values for overdrawn\n",
    "y_true=overdrawn_y\n",
    "# create predicted values of 0/1 for from the model and call the y_pred\n",
    "y_pred=(model2_sm.predict(overdrawn_X)>0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does each of the values in the above confusion matrix represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a visual way to display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "disp.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "From the above confusion matrix, \n",
    "\n",
    "1. calculate accuracy, \n",
    "\n",
    "2. calculate precision, \n",
    "\n",
    "3. calculate recall, \n",
    "\n",
    "4. calculate specificity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "Empty space here.  More code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_true, y_pred)\n",
    "print(f\"Precision: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = specificity_score(y_true, y_pred)\n",
    "print(f\"Specificity: {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(y_true, y_pred)\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do your answers agree with the output from the Python functions?  If not, why not?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
