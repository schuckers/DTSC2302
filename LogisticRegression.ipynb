{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Introduction\n",
    "\n",
    "Logistic regression is a statistical method used for binary classification tasks, where the goal is to predict a categorical outcome that has two classes (e.g., \"yes\" or \"no\", \"spam\" or \"not spam\"). Unlike linear regression, which predicts continuous values, logistic regression predicts the probability of a given input belonging to a certain class.\n",
    "\n",
    "The output of logistic regression is constrained to be between 0 and 1, representing the probability that an observation belongs to a particular class.\n",
    "\n",
    "\n",
    "This function maps any input value to a range between 0 and 1. It is used in logistic regression to convert the output of a linear equation into probabilities.\n",
    "\n",
    "We will use the famous **Iris Dataset** to classify whether a flower belongs to a specific species (Setosa or not).\n",
    "\n",
    "Let's begin by loading the necessary libraries and the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start with some data on irises, the plant.  This data set has four different features/predictors all of which are measurements on the flower of an iris in centimeters.\n",
    "These measurement in order are: _sepal length_, _sepal width_, \t_petal length_ and _petal width_.\n",
    "\n",
    "The data also has species data and there are three species: _Iris Setosa_, _Iris Versicolour_, or _Iris Virginica_ and they are coded as 0, 1, and 2, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.6, 1.4, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. dropdown:: References\\n\\n  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n    Mathematical Statistics\" (John Wiley, NY, 1950).\\n  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n    Structure and Classification Rule for Recognition in Partially Exposed\\n    Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n    on Information Theory, May 1972, 431-433.\\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n    conceptual clustering system finds 3 classes in the data.\\n  - Many, many more ...\\n', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n"
     ]
    }
   ],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "print(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define two functions, logit(p) which takes a probability and returns the log odds, and ilogit which takes log odds and returns a probability.  The latter is the inverse of the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logit(p):\n",
    "    \"\"\"\n",
    "    Calculates the logit of a probability p.\n",
    "\n",
    "    Args:\n",
    "        p (float or numpy.ndarray): Probability value(s) between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float or numpy.ndarray: Logit value(s).\n",
    "    \"\"\"\n",
    "    p = np.asarray(p)\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "def  ilogit(logodds):\n",
    "    ''' \n",
    "    Calculate the inverse logit or inverse log odds of a value.\n",
    "\n",
    "    Args:\n",
    "         float or numpy.ndarray: probability value(s).\n",
    "    '''\n",
    "    logodds = np.asarray(logodds)\n",
    "    return np.exp(logodds)/(1+np.exp(logodds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4054651081081643\n",
      "1.3862943611198908\n",
      "0.5\n",
      "0.11920292202211755\n",
      "0.8807970779778824\n"
     ]
    }
   ],
   "source": [
    "# here are some sample values from these two functions.\n",
    "print(logit(0.4))\n",
    "print(logit(0.8))\n",
    "print(ilogit(0))\n",
    "print(ilogit(-2))\n",
    "print(ilogit(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load in the iris data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.1 4.9 4.7 4.6 5.  5.4 4.6 5.  4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1\n",
      " 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.  5.  5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.\n",
      " 5.5 4.9 4.4 5.1 5.  4.5 4.4 5.  5.1 4.8 5.1 4.6 5.3 5.  7.  6.4 6.9 5.5\n",
      " 6.5 5.7 6.3 4.9 6.6 5.2 5.  5.9 6.  6.1 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1\n",
      " 6.3 6.1 6.4 6.6 6.8 6.7 6.  5.7 5.5 5.5 5.8 6.  5.4 6.  6.7 6.3 5.6 5.5\n",
      " 5.5 6.1 5.8 5.  5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3\n",
      " 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.  6.9 5.6 7.7 6.3 6.7 7.2\n",
      " 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.  6.9 6.7 6.9 5.8 6.8\n",
      " 6.7 6.7 6.3 6.5 6.2 5.9]\n"
     ]
    }
   ],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "# we will start with just the sepal length\n",
    "X = iris.data[:,0]\n",
    "print(X)\n",
    "y = iris.target\n",
    "\n",
    "# For simplicity, let's classify whether the flower is of the species 'Setosa' or not.\n",
    "# \n",
    "y = (y == 0).astype(int)  # 1 if Setosa, 0 otherwise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's create a logistic regression model trying to predict our target, _y_, using the\n",
    "_sepal length_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.843333333333334\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.239455\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  150\n",
      "Model:                          Logit   Df Residuals:                      148\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Mon, 14 Apr 2025   Pseudo R-squ.:                  0.6238\n",
      "Time:                        14:14:03   Log-Likelihood:                -35.918\n",
      "converged:                       True   LL-Null:                       -95.477\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.869e-28\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -2.4148      0.503     -4.800      0.000      -3.401      -1.429\n",
      "x1            -5.1757      0.893     -5.793      0.000      -6.927      -3.425\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(X.mean())\n",
    "# let's center X by subtracting the mean from each value.\n",
    "X = X - X.mean()\n",
    "\n",
    "# for this particular model formulation we need to add a \n",
    "# column of 1's to the feature array\n",
    "#add constant to predictor variables\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit logistic regression model using statsmodels\n",
    "model_sm = sm.Logit(y, X).fit()\n",
    "\n",
    "# Print the summary table, which includes p-values\n",
    "print(model_sm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's intepret these coefficients.  For a logistic regression, the intepretations are similar but the response is the log-odds of the event, in this case the log odds of being an 'iris setosa'.\n",
    "\n",
    "So the '-2.41' is the predicted log odds of an iris that has a mean sepal length (of 5.84 cm) will be of the species 'setosa'.  \n",
    "\n",
    "For each additional cm of length, the predicted log odds of an iris being of the 'setosa' species decreases by -5.18.\n",
    "\n",
    "\n",
    "Next we find the predicted predicted probability of being a 'setosa' for the mean sepal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.08241331812791278)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for a log-odds of -2.41, find the probability\n",
    "ilogit(-2.41)\n",
    "# here that is 0.0824."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's get the predicted probabilities for all of the data.  \n",
    "\n",
    "By default when we use the predict function for our model we get probabilities.  Some other versions of\n",
    "logistic regression (in different python packages) give you predicted log odds, others give you probabilities and others give you the predicted category (here 0 or 1) as your output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.807 0.922 0.971 0.982 0.875 0.47  0.982 0.875 0.994 0.922 0.47  0.952\n",
      " 0.952 0.996 0.101 0.158 0.47  0.807 0.158 0.807 0.47  0.807 0.982 0.807\n",
      " 0.952 0.875 0.875 0.714 0.714 0.971 0.952 0.47  0.714 0.346 0.922 0.875\n",
      " 0.346 0.922 0.994 0.807 0.875 0.989 0.994 0.875 0.807 0.952 0.807 0.982\n",
      " 0.598 0.875 0.    0.005 0.    0.346 0.003 0.158 0.008 0.922 0.002 0.714\n",
      " 0.875 0.062 0.038 0.023 0.24  0.001 0.24  0.101 0.014 0.24  0.062 0.023\n",
      " 0.008 0.023 0.005 0.002 0.001 0.001 0.038 0.158 0.346 0.346 0.101 0.038\n",
      " 0.47  0.038 0.001 0.008 0.24  0.346 0.346 0.023 0.101 0.875 0.24  0.158\n",
      " 0.158 0.014 0.807 0.158 0.008 0.101 0.    0.008 0.003 0.    0.922 0.\n",
      " 0.001 0.    0.003 0.005 0.001 0.158 0.101 0.005 0.003 0.    0.    0.038\n",
      " 0.    0.24  0.    0.008 0.001 0.    0.014 0.023 0.005 0.    0.    0.\n",
      " 0.005 0.008 0.023 0.    0.008 0.005 0.038 0.    0.001 0.    0.101 0.001\n",
      " 0.001 0.001 0.008 0.003 0.014 0.062]\n",
      "[1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "# get predicted probabilities of being a 1 \n",
    "# which here represents being an 'iris setosa'\n",
    "pred_probs=np.round(model_sm.predict(X),3)\n",
    "print(pred_probs)\n",
    "\n",
    "# convert probabilities to ones and zeros\n",
    "pred_cat = (pred_probs>0.5).astype(int)\n",
    "print(pred_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll look at a new dataset about whether or not students overdraw (take too much out of their bank account)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 437 entries, 0 to 449\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Age        437 non-null    float64\n",
      " 1   Sex        437 non-null    float64\n",
      " 2   DaysDrink  437 non-null    float64\n",
      " 3   Overdrawn  437 non-null    float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 17.1 KB\n"
     ]
    }
   ],
   "source": [
    "# read in the monkey data\n",
    "overdrawn = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2302/Overdrawn.csv\")\n",
    "# get info about these data\n",
    "overdrawn.dropna(inplace=True)\n",
    "overdrawn.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is about students.  Here are the details on the variables in these data:\n",
    "\n",
    "_Age_ =\tAge of the student (in years)\n",
    "\n",
    "_Sex_ =\t0=male or 1=female\n",
    "\n",
    "_DaysDrink_ = Number of days drinking alcohol (in past 30 days)\n",
    "\n",
    "_Overdrawn_\t= Has student overdrawn a checking account? 0=no or 1=yes\n",
    "\n",
    "The first regression that we will do will use _DaysDrink_ as a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     const  DaysDrink\n",
      "0      1.0        3.0\n",
      "1      1.0       20.0\n",
      "2      1.0        6.0\n",
      "3      1.0       10.0\n",
      "4      1.0        0.0\n",
      "..     ...        ...\n",
      "445    1.0        1.0\n",
      "446    1.0        0.0\n",
      "447    1.0        8.0\n",
      "448    1.0        8.0\n",
      "449    1.0        0.0\n",
      "\n",
      "[437 rows x 2 columns]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.371758\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:              Overdrawn   No. Observations:                  437\n",
      "Model:                          Logit   Df Residuals:                      435\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Mon, 14 Apr 2025   Pseudo R-squ.:                 0.02897\n",
      "Time:                        14:14:03   Log-Likelihood:                -162.46\n",
      "converged:                       True   LL-Null:                       -167.30\n",
      "Covariance Type:            nonrobust   LLR p-value:                  0.001850\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -2.3332      0.209    -11.178      0.000      -2.742      -1.924\n",
      "DaysDrink      0.0541      0.017      3.201      0.001       0.021       0.087\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "overdrawn_X = overdrawn[['DaysDrink']]\n",
    "overdrawn_X = sm.add_constant(overdrawn_X)\n",
    "\n",
    "overdrawn_y = overdrawn['Overdrawn']\n",
    "print(overdrawn_X)\n",
    "model2_sm = sm.Logit(overdrawn_y, overdrawn_X).fit()\n",
    "\n",
    "# Print the summary table, which includes p-values\n",
    "print(model2_sm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a similar summary to what we had previously for linear regression.  And we have coefficients to interpret.\n",
    "\n",
    "So for a student who had zero drinks in the last 30 days, we would expect that the log odds that they would be overdrawn to be -2.33.\n",
    "\n",
    "For each additional day that a student drank in the last 30 days, we would predict that the log odds of being overdrawn increase by 0.0541.\n",
    "\n",
    "Why might we not want to center the data here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     const  Sex   Age\n",
      "0      1.0  1.0  19.0\n",
      "1      1.0  0.0  19.0\n",
      "2      1.0  0.0  19.0\n",
      "3      1.0  1.0  19.0\n",
      "4      1.0  1.0  19.0\n",
      "..     ...  ...   ...\n",
      "445    1.0  1.0  19.0\n",
      "446    1.0  0.0  18.0\n",
      "447    1.0  1.0  20.0\n",
      "448    1.0  1.0  19.0\n",
      "449    1.0  0.0  20.0\n",
      "\n",
      "[437 rows x 3 columns]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366952\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:              Overdrawn   No. Observations:                  437\n",
      "Model:                          Logit   Df Residuals:                      434\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Mon, 14 Apr 2025   Pseudo R-squ.:                 0.04152\n",
      "Time:                        14:14:03   Log-Likelihood:                -160.36\n",
      "converged:                       True   LL-Null:                       -167.30\n",
      "Covariance Type:            nonrobust   LLR p-value:                 0.0009619\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -7.0458      1.897     -3.714      0.000     -10.764      -3.328\n",
      "Sex            0.9671      0.323      2.990      0.003       0.333       1.601\n",
      "Age            0.2285      0.093      2.458      0.014       0.046       0.411\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "overdrawn_X = overdrawn[['Sex','Age']]\n",
    "overdrawn_X = sm.add_constant(overdrawn_X)\n",
    "\n",
    "overdrawn_y = overdrawn['Overdrawn']\n",
    "print(overdrawn_X)\n",
    "model2_sm = sm.Logit(overdrawn_y, overdrawn_X).fit()\n",
    "\n",
    "# Print the summary table, which includes p-values\n",
    "print(model2_sm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret these coefficients.  \n",
    "\n",
    "First our intercept, -7.05, is the predicted log odds of an age zero male being overdrawn.\n",
    "\n",
    "Next, the coefficient for _Sex_ is 0.9671 which means that a female will have a predicted log odds of being overdrawn that is 0.97 higher than a male with the same age.\n",
    "\n",
    "For each additional year of age, we expect that the log odds of being overdrawn will be predicted to increase by 0.23 assuming that _Sex_ remains the same.\n",
    "\n",
    "Below we center age so that our intercept coefficient is more interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.622425629290618\n",
      "     const  Sex       Age\n",
      "0      1.0  1.0 -0.622426\n",
      "1      1.0  0.0 -0.622426\n",
      "2      1.0  0.0 -0.622426\n",
      "3      1.0  1.0 -0.622426\n",
      "4      1.0  1.0 -0.622426\n",
      "..     ...  ...       ...\n",
      "445    1.0  1.0 -0.622426\n",
      "446    1.0  0.0 -1.622426\n",
      "447    1.0  1.0  0.377574\n",
      "448    1.0  1.0 -0.622426\n",
      "449    1.0  0.0  0.377574\n",
      "\n",
      "[437 rows x 3 columns]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366952\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:              Overdrawn   No. Observations:                  437\n",
      "Model:                          Logit   Df Residuals:                      434\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Mon, 14 Apr 2025   Pseudo R-squ.:                 0.04152\n",
      "Time:                        14:14:03   Log-Likelihood:                -160.36\n",
      "converged:                       True   LL-Null:                       -167.30\n",
      "Covariance Type:            nonrobust   LLR p-value:                 0.0009619\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -2.5626      0.275     -9.306      0.000      -3.102      -2.023\n",
      "Sex            0.9671      0.323      2.990      0.003       0.333       1.601\n",
      "Age            0.2285      0.093      2.458      0.014       0.046       0.411\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(overdrawn['Age'].mean())\n",
    "overdrawn['Age']=overdrawn['Age']-overdrawn['Age'].mean()\n",
    "overdrawn_X = overdrawn[['Sex','Age']]\n",
    "overdrawn_X = sm.add_constant(overdrawn_X)\n",
    "overdrawn_y = overdrawn['Overdrawn']\n",
    "print(overdrawn_X)\n",
    "model2_sm = sm.Logit(overdrawn_y, overdrawn_X).fit()\n",
    "\n",
    "# Print the summary table, which includes p-values\n",
    "print(model2_sm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that after centering _Age_ the coefficients for _Sex_ and _Age_ remain the same, but the coefficient for _const_ changes.  \n",
    "\n",
    "From the above code, we can see that the average age of students in these data was 19.6 years of age.  Having subtracted that from _Age_, we now intepret the y-intercept as the predicted log odds that\n",
    "a 19.6 year old male would overdraw from their bank account is -2.56.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. Using the overdrawn data, fit a model that has _Age_, _Sex_ and as an interaction term between 'Age' and 'Sex'.  Interpret all of the coefficients.  \n",
    "\n",
    "2. Again with the overdrawn data, fit a model with all of the predictors including the interaction term from the previous task.  Which of the predictors is discernibly different from zero.\n",
    "\n",
    "3. Using the model from Task 2, remove any non-discernibly different from zero predictors from the model and get the predicted probabilities of being overdrawn.  \n",
    "\n",
    "4. Convert the predicted probabilities in the previous task to whether or not a student was overdrawn.  Is the predicted number of overdrawn students the same as the actual number of overdrawn students?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
