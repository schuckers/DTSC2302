{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance and SHAP values\n",
    "\n",
    "In this Jupyter Notebook, we will\n",
    "\n",
    "### Feature Importance\n",
    "\n",
    "We will start with feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the _iris_ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load sample dataset (Iris Dataset for classification)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the dataset for models that are sensitive to feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"SVM\": SVC(kernel='linear', random_state=42),  # Linear SVM for simplicity\n",
    "    \"LDA\": LinearDiscriminantAnalysis(),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate feature importance for each classifier\n",
    "def calculate_feature_importance(classifiers, X_train, y_train):\n",
    "    importance_results = {}\n",
    "\n",
    "    for name, clf in classifiers.items():\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        if name == \"Decision Tree\":\n",
    "            importance_results[name] = clf.feature_importances_\n",
    "        \n",
    "        elif name == \"Logistic Regression\":\n",
    "            # Coefficients of Logistic Regression (scaled by regularization)\n",
    "            importance_results[name] = np.abs(clf.coef_[0])\n",
    "        \n",
    "        elif name == \"SVM\":\n",
    "            # SVM uses the absolute value of coefficients as feature importance\n",
    "            importance_results[name] = np.abs(clf.coef_[0]) if hasattr(clf, 'coef_') else None\n",
    "        \n",
    "        elif name == \"LDA\":\n",
    "            # For LDA, the absolute values of the coefficients are used to determine importance\n",
    "            importance_results[name] = np.abs(clf.coef_[0])\n",
    "    \n",
    "    return importance_results\n",
    "\n",
    "# Calculate feature importance\n",
    "importance_values = calculate_feature_importance(classifiers, X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot feature importance\n",
    "def plot_feature_importance(importance_values, feature_names):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for i, (name, importance) in enumerate(importance_values.items()):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.barh(feature_names, importance)\n",
    "        plt.title(f'Feature Importance - {name}')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Feature names from the Iris dataset\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Plotting the feature importances\n",
    "plot_feature_importance(importance_values, feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the impact of the four features on these four models types.  \n",
    "\n",
    "For the decision tree model, clearly petal length is the most important feature.  For SVM, petal \n",
    "length and petal width seem to be more important than sepal width and sepal length.  For LDA, petal length is more important than petal width which is more important that sepal width which is more important than sepal length.  Finally, all of the features seem relatively important for the logistic regression with petal length being the most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the bcancer data\n",
    "bcancer = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2301/Data/BreastCancer.csv\", na_values=['NA'])\n",
    "bcancer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=bcancer[['Radius', 'Texture','Perimeter','Area','Smoothness','Compactness',\n",
    "           'Concavity','Concave Points','Symmetry']]\n",
    "\n",
    "y=bcancer['Diagnosis']\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the dataset for models that are sensitive to feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"SVM\": SVC(kernel='linear', random_state=42),  # Linear SVM for simplicity\n",
    "    \"LDA\": LinearDiscriminantAnalysis(),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance\n",
    "importance_values = calculate_feature_importance(classifiers, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature names from the Iris dataset\n",
    "feature_names = ['Radius', 'Texture','Perimeter','Area','Smoothness','Compactness',\n",
    "           'Concavity','Concave Points','Symmetry']\n",
    "\n",
    "# Plotting the feature importances\n",
    "plot_feature_importance(importance_values, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP\n",
    "\n",
    "The SHAP approach to determining the impact of a feature is to \n",
    "consider at all possible combinations of features and how the prediction would change \n",
    "if a feature were included or excluded. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### !!!! you likely have to install shap\n",
    "# Try pip3 install shap\n",
    "######\n",
    "# If that doesn't work try pip3 install shap --pre\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Logistic Regression Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Create a SHAP explainer object\n",
    "explainer = shap.Explainer(model, X_train_scaled)\n",
    "\n",
    "# Get SHAP values for the test set\n",
    "shap_values = explainer(X_test_scaled)\n",
    "\n",
    "# Plot the SHAP summary plot (global explanation of the model)\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of impact for these features seems to be 2, 0, 7, 1, 8, and 4 followed by 5, 3 and 6.  The latter three seems to have a good bit less impact than the former six.  Recall that the order of the \n",
    "feature names was ['Radius', 'Texture','Perimeter','Area','Smoothness','Compactness','Concavity','Concave Points','Symmetry'], so that Perimeter, feature 2, would be the most impactful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Support Vector Machine Classifier\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Create a SHAP explainer object\n",
    "explainer = shap.Explainer(model, X_train_scaled)\n",
    "\n",
    "# Get SHAP values for the test set\n",
    "shap_values = explainer(X_test_scaled)\n",
    "print(shap_values)\n",
    "# Plot the SHAP summary plot (global explanation of the model)\n",
    "shap.summary_plot(shap_values, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above plot, we can see that feature 2, 0, 1, 7, and 8 seem to be important with positive relationships with the response.  Features 5, 6, and 3 seem to have less impact with Features 5 and 3 having a negative relationship with our response here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. Choose one of the model types from the Feature Importance section above and fit the model with all of the predictors.  Assess that model for cross-validation accuracy.\n",
    "\n",
    "2. For the model you chose in the previous task, look at the Feature Importance plot and remove some of the variables from the full model in the previous task.  Fit this new smaller model and assess that model using cross-validation.  Was the cross-validation accuracy an improvement.\n",
    "\n",
    "3.  Get the SHAP values for the features in a linear discriminant analysis.  \n",
    "\n",
    "4. Using the output you got from the previous task, create a reduced model with fewer predictors and evaluate that model via cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
